Basically, why Kubernetes?
The Docker had many problems in day-to-day life in the industry,
Four main problems of the docker that are,
    •	Single host
    •	Not capable of Auto scaling
    •	Not capable of Auto Healing
    •	Docker is not of Enterprise Nature
So, if we wonder how all these problems are solved, here comes the Kubernetes.
Kubernetes is an Open-source platform for managing containerized applications. It automates the deployment, scaling and management of these applications. It is portable, extensible and declarative, and has a large and growing ecosystem of services and tools. 
So, Kubernetes came up with a solution for all those docker problems.

How did it solve the problem of single host?
By default, Kubernetes is a cluster, cluster is group of nodes. So, Kubernetes on production is installed in terms of Master node architecture just like Jenkins. 
If there are 100s of containers running and if one of the container lifecycles is affecting any of the containers, what Kubernetes does is, it just picks up the affected container and puts into the other node to resume its capability without any intervention.

How does it solve Auto scaling?
Kubernetes has something called replica set.
So, if a container is experiencing a huge load on a certain time (the no of users hitting the application n increases). We have replicaset.yaml file and increase the replicas and bring up the new containers manually. Kubernetes also supports HPA (Horizontal pod Auto scaler) using which we can increase the no of containers whenever we experience the load.

How does it solve Auto healing?
Kubernetes controls and fix the damage.
If one of the containers goes down, using the Auto healing feature Kubernetes will spin up a new container before the old one dies.
We have something called API server in k8s, this API servers receives the signal of container going down, immediately the k8s rolls out a new container before the old one goes down. The user doesn’t even experience of any shutdown of the application.

How will it support Enterprise Nature?
People at Google Built Enterprise level container orchestration platform called Kubernetes.
Since Docker is not of enterprise nature (Load balancer support, firewall support, auto healing, auto scaling, Api gateways, blacklisting ips etc.). so, it is never used in Production platform. Kubernetes is aiming to solve these problems.

Kubernetes Architecture
Master Node (Control Plane) 	Worker Node (Data plane)
API Server	                    Kubelet
Scheduler	                    Kubeproxy
etcd	                        Container Runtime
Controller Manager	
CCM (Cloud Controller manager)	

To explain this, in docker simplest thing is container and k8s simplest thing is pod.
When a container is created in docker using a docker run command, what happens behind the scenes. when we use docker run command, to execute this process we should have container run time env which is called Dockershim.
Similarly in Kubernetes, we have master node architecture. When a pod is deployed on k8s it goes through Master node and executed on worker node.

Worker Node
Kubelet – this is responsible for running/maintaining a pod. If the pod goes down it reminds a component in control plane to fix or control this issue.
Container Runtime - in order to run a pod in k8s we should have a container runtime and here unlike docker, docker is not mandatory. We can user either dockershim or containerd or crio(all these are container runtimes)
Kubeproxy – Like default bridge network(docker0) in Docker, this kube-proxy basically provides us the networking, it allocates the ips to every pods we are creating and also provides the load balancing capability since, k8s supports auto scaling capability. It uses IP tables in Linux.

Master Node
API sever – Acts as a core component in K8s and takes all incoming requests. It basically exposes K8s to the external world.
Scheduler – it is responsible for scheduling the pods/resources on the K8s. for example, it schedules the pod1 to run on one of the available nodes.
etcd – it is key-value storage, and the entire cluster related information is stored as objects/ key-value pairs inside etcd. It helps in restoring the cluster related information when required.
Controller manager – since k8s supports auto scaling therefore k8s has something called as Controllers. For example, replica sets. This replica sets is maintaining state of K8s and help in auto scaling.
CCM – K8s can be run on different cloud platforms. When a user requests to create load balancer on EKS to k8s. Firstly k8s must understand the underlying cloud providers and translate the user request on to the API request that the cloud provider must understand. This mechanism is implemented on the CCM. So that load balancer gets created on the EKS.

Each Configuration file has 3 parts:
1. Metadata
2. Specification
3. Status - this one is automatically generated and added by Kubernetes.

Layers of Abstractions:
when you create a deployment, by default it creates replica set and a pod. Therefore,
Deployment manages a ReplicaSet
ReplicaSet manages a Pod
Pod is an abstarction of Container

Connecting Components:
The connection is established by Labels, selectorsand ports.  
Metadata part contains Labels and Specification part contains Selectors

What is Namespace?
Organise Resources in namespace. It is a virtual cluster inside a cluster.
a cluster contains 4 namespaces : 
kube-system: do not create or modify in kube-system. system process, master and kubectl are deployed in this namespace.
kube-public: contains publicely accesble data like configmap, which contains cluster information
kube-node-lease: holds the heartbeats of nodes, determines the availability of nodes
default: resources you create without any particular namespace are located here. 

Use cases of namespace:
1. Resource grouped in Namespaces : logically grouping of resources inside a cluster
2. Confilcts: many teams, same application : each team can work in their own namespace without disrupting others.
3. Resource Sharing: Staging and development : namespaces can hold the common resources and these resouces can be shared across staging and development, rathar than creating in both of them.
4. Resource Sharing: Blue/Green Deployment: (Blue/green deployment: it is live production and coming up production instances) these both production instances can use common resources deployed under a namespace.
5> Access and resource Limits on Namespaces : limits authentication to the users based on the namespace. It also limits the available resource to the namespaces in the cluster.

Characterstics of a namespace:
1. Each NS must define own ConfigMap.
2. each NS must define own Secrets.
3. Service can be shared across Namespace
4. Some Components, which cant be created within namespace are volume, node.

Service:
Why service is needed?
so in the world of kubernetes, when the pod goes down and comes back due to its auto healing capability, the pod gets a different ip address every time a pod goes down and comes back, so a user cant access the application using Ip address. therefore kubernetes introduces a service for this purpose and application can be accessed via service,

The service provides the capabilities like:
1. Load balancing: It distributes the incoming requests to the available pods equally
2. Service Discovery: Services uses Labels and Selectors to talk to the pods instead of Ip addresses.
3. Exposes the application to the external Network: The service helps in exposing the application running inside the pods to the external network in 3 different ways i.e. 1) Cluster IP 2) Node Port 3) Load balancer

1. Cluster IP: this mode of service will allow only people having access to cluster can access the application.
2. Node port : this mode of service allows only people having access to the node where application resides ( basically people inside the organisation)
3. Load balancer: this mode of service exposes the application to the internet, where it creates a public IP address.

While accessing the application from the outside world, the kubernetes generates an external ip, you can access the application via http://<external ip>:<service_port>
while accessing the application using Node ip, you can use the Node ip address by the command kubectl get nodes -o wide, like http://<node_ip>:<node_port>. To access the particular node_port you need to allow the port access in the firewall settings. 
gcloud compute firewall-rules create test-node-port \
    --allow tcp:NODE_PORT

Ingress:
without ingress in the kubernetes, the application did not have the luxuries of Enterprise application like:
1. Ratio based LoadBalancing : 40% of req has to go to one pod and 60% of req has to go to other
2. Sticky sessions - particular user traffic has to reach particular pod
3. Path based loadbalancing
4. Domain Based Load Balancing
5. Black and white listing

Another problem was, if the application had some 1000 microservices then the service type of LoadBalancer will have 1000s of static public ip address which the cloud provider will create, so tthis would bear a huge cost on the company.

Ingress Controller:
This is a component which looks for a Ingress resouce. This is just Load Balancer and Api Gateway.
To install the ingress controller: https://kubernetes.github.io/ingress-nginx/deploy/#gce-gke
To check the logs of the ingress controller : kubectl logs <pod name> -n <namespace>
to configure the cloud DNS watch: https://www.youtube.com/watch?v=1lsFYHtWbEc





